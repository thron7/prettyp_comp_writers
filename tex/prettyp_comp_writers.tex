\documentclass[11pt,a4paper]{article}
\usepackage{multicol}
\usepackage{fullpage}

\begin{document}
\title{Pretty-Printing for Compiler Writers}
\author{
Thomas Herchenroeder\\
1\&1 Internet AG
}

\date{\today}
\maketitle

%\begin{multicols}{2}
\begin{abstract} 
Pretty-printing, i.e. the regularly formatting of source code
in terms of line breaks and indentation, poses extra challenges to compiler
writers, namely handling of code comments. This experience report discusses problems and
approaches to address these. Particularly, it proposes to use a \emph{Concrete
Syntax Tree} to capture and maintain code comments, which are usually not
included in parse results. But they are needed to restore the text of parsed
source code faithfully.
\end{abstract}

\section{Introduction}
Pretty-printing is a little discussed issue, although it is not
far-fetched. Many IDEs provide automatic code layout, as well do many command
line tools. But it poses a significant
challenge: the handling of code comments. Parser and compiler writers
frequently waive their hands, as comments in code are treated as white space and
are usually ignored during parsing.

\subsection{Context}

Pretty-printing can be approached in several ways, and there are several
contexts where pretty-printing code might be desirable. This paper assumes that
the goal of pretty-printing is a modified serialized form of the initial source
code, usually as a file on hard disk.This is crucial as not having to serialize
at the end (like in an interactive editor or IDE) might lead to different design
decisions.

So we picture ourselves in a workflow of \emph{reading code in --
processing it -- and writing it out again}, like typical command-line tools
would do. Within this context there are again multiple ways to address the
\emph{"processing it"} part. Parsing the source code seems to make sense, as we
want to base text layout decisions on syntactical correctness, like keeping two
adjacent tokens on the same line when they would have different semantics when
split across two. So we have to recognize grammatical structures.

Then the question arises what to do with the parsed tokens (or technically
speaking, which parse actions to deploy). One approach would be to serialize the
tokens back immediately, applying whitespace as we go along. While tempting and
with its own strengths, this approach becomes tedious as soon as you have to handle
deeply nested structures where you want to match begin and end actions.

So we came out constructing a parse, or syntax, tree, which is a common parse
result. The serial source text is transfered into a hierarchical tree structure
where nodes represent atomic code fragments, or \emph{lexemes}.

\subsection{Application background}

As a background information, this report is based on experiences gleaned from
writing a custom pretty-printer for the JavaScript programming language. The
implementation language for the pretty-printer was Python. But all results are
general enough that they should be applicable to different implementation and
object languages, as long as the object language has a comparable way to compose
comments.

\subsection{JavaScript comments - a quick look}

JavaScript has C++-like comments. As a reminder, here are the two ways you
can add comments in source code:

\textit{Block comments}

Block comments start with \texttt{/*} and are closed with \texttt{*/}. Nesting
block comments is not allowed, but a closing tag can be embedded by escaping the
asterisk, \texttt{\\*/}.

Examples:

\begin{verbatim}
  /* foo bar baz */

  var a = 3; /* block comment at end of line */

  var b = 4; /* dangling off code
     and spanning multiple
     lines */

  var c /* in the middle */ = /* of some expression */ 5;

\end{verbatim}

\textit{Line-end comments}

Line-end comments start with a double forward slash, \texttt{//}, and extend to
the end of line, i.e. to the next newline character.

Examples:

\begin{verbatim}

  // foo bar baz
  
  var a = 3; // line-end comment

  // no spreading across lines or directly embedding in code
  var b = 4;

\end{verbatim}

\section{Conventions}

In the remainder of this report I will use the following conventions:

\begin{itemize}
\item In order to distinguish comments from the "real" code, I
will refer to latter as \emph{syntactical} elements, meaning they belong to the
syntax of the programming language proper. I hope this is not offending people.

\item Occasionally, I will represent trees as s-expressions. Subtrees will be
enclosed in parentheses, the root nodes of any subtree will be the first symbol
in the s-expression (its "car"), children will be the remainder of the
s-expression (its "cdr"). Separation is by whitespace. So the tree for the
expression \texttt{3 + 4 * 5} will be represented as \texttt{(+ 3 (* 4 5))}.
This is not particularly pleasant to read, but the easiest way to write them
$\ddot\smile$.
\end{itemize}



%   --------------------------------------------------------------------------
% - APPROACHING THE ISSUE
%   --------------------------------------------------------------------------

\section{Approaching the Issue}

\subsection{Using an Abstract Syntax Tree}

An abstract syntax tree (\emph{AST}) captures the essential structure of a
concrete piece of code. It groups tokens together in subtrees, and composes
those subtrees to represent larger constructs until it eventually captures the
entire source code, e.g. the contents of a source file. ASTs typically leave out
information that is implicit, and can be inferred from the AST later on. For
example, in a language like JavaScript, the list of formal parameters in a
function declaration is a list of legal identifiers, separated by commas, and
enclosed by parentheses.

\begin{verbatim}
   function foo (a,b,c) { ... }
\end{verbatim}

In a tree representation, this list of formal parameters can be represented
without the parens and the commas, like \texttt{(params (a b c))}. The
(synthetic) tree node \emph{params} has three identifier children, \emph{a},
\emph{b} and \emph{c}. That's it, just having the root node named "param"
suffices to indicate that in the original source parentheses and commas where
present.

But using such an abstract tree poses two problems. Firstly, the parse
introduces synthetic nodes, i.e. nodes which do not represent lexemes. As the
\emph{next} function can only attach comments to tokens actually found in the
code, the parser needs to make sure that comments coming piggy-back from
\emph{next} get hooked up elsewhere. Otherwise they are lost when e.g. they come
with the opening parenthesis token as this is discarded in the tree.

Secondly, even if all comments are carried over to the AST, you run into
undecidable situations when trying to serialize them out again. As an example,
take the case of a block comment in an empty list. In the AST the only place to
attach the comment is with the list node that has no children, or with the
parent construct the list is part of (Remember that
comments are not part of the AST itself, so are not first class children or
siblings of
other nodes,  but are attached via a separate property to proper AST nodes).

[Explanation not good]

From the standpoint of the AST these constructs become indistinguishable.

\begin{verbatim}

  var a = [ /* Put stuff in here! */ ];

  var a = [] /* Put stuff in here! */ ;

\end{verbatim}

but ostensibly with implications for the semantics of the comment.

As we saw, closing brackets of any kind of construct are
particularly prone to these kinds of problems.


\subsection{The Tokenizer needs to provide Comments as Tokens}

It might not all too obvious, so I am stating it here explicitly. The tokens the
parser receives from the tokenizer are high-level. It produces not only
elementary lexemes like keywords, operators or number literals, but also the
syntactically more complex elements like string literals, regular expression
literals, line-end and block comments as single tokens.


\subsection{Normalizing Indentation of Comments}


\section{Comments in Syntax Trees}

If you go down the road of parsing the source text into a syntax tree, there is
no other way than storing comment tokens with the syntax tree in some way. For
our purpose here lets assume that the tokenizer is providing the parser with
comments as atomic tokens, potentially annotated with their type (block or
line-end) and further properties like number of lines etc. Lets also assume
that, like in many grammars, a newline character immediately following the
comment is an own token which is not subsumed into the comment (for line-end
comments).

The first and fundamental decision is whether to add comment tokens as
first-class nodes into the syntax tree. While this seems natural and
straight-forward it makes the parsing of code-level constructs like function
definitions or while loops extremely complicated. Basically, every such parse
rule has to account for comments token interspersed into the token stream. A
parse rule for e.g. a "while" construct deteriorates into something saying "if
the current token is "while", the next token must be an opening parenthesis - or
a comment; then comes a condition expression - or a comment; then an opening
curly - or a comment; ..." and so on and so forth. Parsing rules become tedious
and complex, and blur the syntactic structure they are trying to capture. One
might argue that such parsers could be generated, just by adding a flag to the
parser generator ("--include-comments"). For hand-written parsers it's a
nightmare.

\subsection{Associating Comments with Syntax Nodes}

So the next idea is to keep comment tokens out of the syntax tree. But whither
then?! Retaining their relation to the syntactical tokens (which one comes before,
which comes after) is essential to restore this order when later serializing the
tree again. Two options here are attaching the comments as properties to tree
nodes, or maintaining a dedicated table of comments and later inferring their
position by comparing positional information of the tree nodes with those of the
comment entries (see \cite{bartho-2009}). (Bartho actually uses this information
to insert the comments into the syntax tree once this has been finalized by the
parser, thereby creating a tree where comments are first-class.)

Either way you relate tree nodes to comment nodes in an surjective and
invertible way, i.e. every comment node is associated with exactly one tree
node. Tree nodes, though, may be associated with multiple comment nodes, simply
by the fact that multiple comments can precede or follow a syntactical token in
the source code.

We decided to attach comments to tree nodes, and such that comments are attached
to the next \textbf{following} syntactical token. That means the \textit{next}
function stacks comment tokens until a syntactical token is encountered,
and then attaches the list of stacked comments to it. This has several implications:

\begin{itemize}
\item In order to accommodate all comments of the source, there always
  \textbf{has} to be a next syntactical token. This might not be true if a
  comment appears as the last token in the token stream, representing the end of
  the source code. In this case a synthetic \textit{EOF} token is inserted in
  the tree, to capture all pending comments.
\item Comments are attached to the lexically next syntactical token. But these
  tokens are rarely the root symbols of their respective expression. In a
  snippet like \\
  \begin{verbatim}
    // some comment
    a + b;
  \end{verbatim}
  the comment is attached to \emph{a}, but the expressions root node is
  \emph{+}. As a consequence, locating comments in a subtree often requires
  looking for the left-most child, or more precisely, for the \emph{head
  symbol}, the symbol of this subtree that is lexically first. It is
  recommendable to to unify searching for such a head symbol with the
  serialization logic, as looking for the head symbol amounts to starting to
  serializing the subtree and then stopping after the first lexeme. (Python
  generators lend themselves well to this task as they are lazy.)
\end{itemize}

\section{The best Approximation: Concrete Syntax Trees}

So it seems the best approach is to store comments with the syntax tree, but
also use a concrete syntax tree that retains all lexems from the tokenizer. 

\section{And finally, Pretty-Printing}

With a syntax tree as described above, serializing the tree back to a text form
takes the following steps:

\begin{itemize}
\item You serialize the tree in the usual recursive-descent way, e.g. by using a
visitor pattern.
\item For every node you visit, you check if it has comments attached to it.
\item If so, you serialize these comments first, then the node itself.
\item In serializing the comments, you evaluate current indentation and line
information attributed to the comment tokens, in order to integrate them with
the syntactical code. This might involve some effort, e.g. distinguishing
between block and line-end comments, whether to start them at the current or a
new line, and if they span multiple lines (for block comments), how to indent
subsequent comment lines before the comment ends. Then, normal indentation
must be restored for the next code lexeme.
\end{itemize}

\section{Conclusion}

In closing, if you want to go the road of constructing a parser tree from source
code, and then use this parse tree to pretty-print the code, your best bet is to
employ a concrete syntax tree that captures all lexemes of the token stream. You
can then associate comment tokens with syntactical tokens, without interfering
with the grammatical parsing of the source syntax. 

\bibliographystyle{plain}
\bibliography{prettyp_comp_writers}

%\end{multicols}
\end{document}
